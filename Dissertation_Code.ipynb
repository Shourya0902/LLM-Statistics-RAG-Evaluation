{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af68785b",
   "metadata": {},
   "source": [
    "# Can Large Language Models Learn Statistics? A RAG Experiment\n",
    "**Author:** Shourya Marwaha  \n",
    "**Context:** MSc Dissertation Project\n",
    "\n",
    "## üìÑ Abstract\n",
    "This project investigates whether Large Language Models (LLMs) genuinely \"learn\" and integrate new statistical concepts when provided with external textbooks via **Retrieval Augmented Generation (RAG)**. \n",
    "\n",
    "Using a **$2\\times3$ factorial design**, I evaluated three distinct model architectures (Llama 3.2, DeepSeek-R1, Qwen2-Math) across 50 graduate-level statistics questions. The study utilizes a custom **\"LLM-as-a-Judge\"** evaluation framework to assess Correctness, Explanation Quality, and Understanding.\n",
    "\n",
    "## üõ†Ô∏è Technical Methodology\n",
    "* **RAG Pipeline:** ChromaDB (Vector Store) + Cross-Encoder Reranking (`ms-marco-MiniLM-L-6-v2`)\n",
    "* **Embeddings:** `all-mpnet-base-v2`\n",
    "* **Evaluation:** Automated rubric grading using a Judge LLM validated against human consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & Configuration ---\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Set\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "import glob\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP & AI\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import chromadb\n",
    "\n",
    "# LangChain & LlamaIndex\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# PDF Processing\n",
    "from pdf2image import convert_from_path\n",
    "from pytesseract import pytesseract\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Setup ---\n",
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PATH\"] += os.pathsep + '/opt/homebrew/bin'\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# Download NLTK data\n",
    "nltk_packages = [\"wordnet\", \"omw-1.4\", \"stopwords\", \"punkt\"]\n",
    "for pkg in nltk_packages:\n",
    "    nltk.download(pkg, quiet=True)\n",
    "\n",
    "# Initialize Clients\n",
    "ollama_client = ollama.Client()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# Data Paths\n",
    "dataset_books = os.getenv(\"DATASET_BOOKS\")\n",
    "dataset_chunk = os.getenv(\"DATASET_CHUNK\") \n",
    "dataset_main = os.getenv(\"DATASET_MAIN\")\n",
    "\n",
    "# Initialize Models\n",
    "# Using all-mpnet-base-v2 for robust semantic embeddings\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# Using CrossEncoder for high-precision reranking\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# Database Setup\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"stats_books\")\n",
    "\n",
    "print(\"‚úÖ Environment Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Document Processing Pipeline ---\n",
    "\n",
    "def load_documents(data_path):\n",
    "    \"\"\"Loads PDF documents from the specified directory.\"\"\"\n",
    "    # Find all PDF files\n",
    "    pdf_files = glob.glob(os.path.join(data_path, \"**/*.pdf\"), recursive=True)\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for file_path in pdf_files:\n",
    "        file_name = Path(file_path).name\n",
    "        print(f\"   üìñ Loading: {file_name}\")\n",
    "    \n",
    "        loader = UnstructuredPDFLoader(file_path)\n",
    "        file_docs = loader.load()\n",
    "            \n",
    "        for doc in file_docs:\n",
    "            doc.metadata['source_file'] = file_name\n",
    "            documents.extend(file_docs)\n",
    "            print(f\"   ‚úÖ Loaded {len(file_docs)} chunks from {file_name}\")\n",
    "    \n",
    "    print(f\"\\nüìä Total: {len(documents)} chunks from {len(pdf_files)} files\")\n",
    "    return documents\n",
    "\n",
    "def split_text(documents, chunk_size, overlap):\n",
    "    \"\"\"Splits documents into smaller chunks for vector storage.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    length_function=len,\n",
    "    separators=[ \"\\n\\n## \",  \"\\n\\n# \",  \"\\n\\nExample\",  \"\\n\\n\",  \"\\n\",  \". \", \" \"  ])                           \n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "    \n",
    "# Load the data\n",
    "# documents = load_documents(dataset_books) # Uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Query Transformations (Multi-Query Expansion) ---\n",
    "\n",
    "_STOP = set(stopwords.words(\"english\"))\n",
    "_LEM  = WordNetLemmatizer()\n",
    "\n",
    "def _normalize_tokens(text: str) -> List[str]:\n",
    "    return re.findall(r\"[A-Za-z0-9]+\", text.lower())\n",
    "\n",
    "def _lemmatize(tokens: List[str]) -> List[str]:\n",
    "    return [_LEM.lemmatize(t) for t in tokens]\n",
    "\n",
    "def condense_query(query: str) -> str:\n",
    "    \"\"\"Simplifies the query to keywords.\"\"\"\n",
    "    toks = _normalize_tokens(query)\n",
    "    toks = [t for t in toks if t not in _STOP]\n",
    "    toks = _lemmatize(toks)\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def synonym_variants(query: str, max_per_term: int = 2, max_variants: int = 3) -> List[str]:\n",
    "    \"\"\"Generates synonymous queries using WordNet.\"\"\"\n",
    "    toks = _normalize_tokens(query)\n",
    "    content = [t for t in toks if t not in _STOP and len(t) > 2]\n",
    "    variants: Set[str] = set()\n",
    "    if not content:\n",
    "        return []\n",
    "    for term in content[:6]:\n",
    "        syns = []\n",
    "        for syn in wn.synsets(term):\n",
    "            for lemma in syn.lemmas():\n",
    "                w = lemma.name().replace(\"_\", \" \").lower()\n",
    "                if w != term and w.isalpha() and len(w) > 2:\n",
    "                    syns.append(w)\n",
    "        counts = Counter(syns)\n",
    "        for alt, _ in counts.most_common(max_per_term):\n",
    "            new_toks = [alt if t == term else t for t in content]\n",
    "            variants.add(\" \".join(new_toks))\n",
    "            if len(variants) >= max_variants:\n",
    "                break\n",
    "        if len(variants) >= max_variants:\n",
    "            break\n",
    "    return list(variants)\n",
    "\n",
    "def generate_multi_queries(query: str, max_variants: int = 5) -> List[str]:\n",
    "    \"\"\"Combines original, condensed, and synonym queries.\"\"\"\n",
    "    q0 = query.strip()\n",
    "    q1 = condense_query(q0)\n",
    "    q_syns = synonym_variants(q0, max_per_term=2, max_variants=max(0, max_variants - 2))\n",
    "    seen = []\n",
    "    for q in [q0, q1] + q_syns:\n",
    "        if q and q not in seen:\n",
    "            seen.append(q)\n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieval System ---\n",
    "\n",
    "def clear_and_create_collection():\n",
    "    try:\n",
    "        chroma_client.delete_collection(name=\"stats_books\")  # Consistent name\n",
    "        print(\"Cleared existing collection\")\n",
    "    except Exception as e:\n",
    "        print(f\"No existing collection to clear ({e})\")\n",
    "    global collection\n",
    "    collection = chroma_client.get_or_create_collection(name=\"stats_books\")  # Same name\n",
    "    print(\"Created fresh collection\")\n",
    "\n",
    "def retrieve_relevant_chunks(query, top_k):\n",
    "    \"\"\"Basic retrieval using cosine similarity.\"\"\"\n",
    "    query_embedding = embedder.encode([query])[0]\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],  \n",
    "        n_results=min(top_k * 2, 15),  # Get extra candidates\n",
    "        include=[\"documents\", \"distances\"])\n",
    "    \n",
    "    documents = results.get(\"documents\", [[]])[0]\n",
    "    return documents[:top_k]\n",
    "\n",
    "def retrieve_pool_with_multiquery(query: str, collection, embedder, k_per_query: int = 8, verbose: bool = True):\n",
    "    \"\"\"Retrieves candidates using multiple query variations.\"\"\"\n",
    "    multi_queries = generate_multi_queries(query, max_variants=5)\n",
    "    if verbose:\n",
    "        print(\"Transformed queries:\")\n",
    "        for i, mq in enumerate(multi_queries, 1):\n",
    "            print(f\"  {i}. {mq}\")\n",
    "\n",
    "    pool: Dict[str, Dict[str, Any]] = {}\n",
    "    for mq in multi_queries:\n",
    "        q_emb = embedder.encode([mq])[0].tolist()\n",
    "        res = collection.query(\n",
    "            query_embeddings=[q_emb],\n",
    "            n_results=int(k_per_query),\n",
    "            include=[\"documents\", \"metadatas\", \"distances\", \"ids\"] # Ensure IDs are included\n",
    "        )\n",
    "        docs = res.get(\"documents\", [[]])[0]\n",
    "        metas = res.get(\"metadatas\", [[]])[0]\n",
    "        dists = res.get(\"distances\", [[]])[0]\n",
    "        ids   = res.get(\"ids\", [[]])[0]\n",
    "\n",
    "        for _id, d, m, dist in zip(ids, docs, metas, dists):\n",
    "            if _id not in pool or dist < pool[_id][\"distance\"]:\n",
    "                pool[_id] = {\"id\": _id, \"doc\": d, \"meta\": m, \"distance\": float(dist)}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nCandidate pool size (deduped): {len(pool)}\")\n",
    "    return list(pool.values())\n",
    "\n",
    "def rerank_candidates(query: str, candidates: list, top_k: int = 4, verbose: bool = True):\n",
    "    \"\"\"Reranks candidates using CrossEncoder.\"\"\"\n",
    "    if not candidates:\n",
    "        if verbose:\n",
    "            print(\"No candidates to rerank.\")\n",
    "        return []\n",
    "    pairs = [(query, c[\"doc\"]) for c in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    for c, s in zip(candidates, scores):\n",
    "        c[\"rerank_score\"] = float(s)\n",
    "    candidates.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "    top = candidates[:top_k]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nTop {top_k} after rerank:\")\n",
    "        for i, c in enumerate(top, 1):\n",
    "            # Printing meta data logic kept from your code\n",
    "            meta_str = \"\"\n",
    "            if isinstance(c.get(\"meta\"), dict):\n",
    "                src = c[\"meta\"].get(\"source\") or c[\"meta\"].get(\"file\") or c[\"meta\"].get(\"path\")\n",
    "                page = c[\"meta\"].get(\"page\") or c[\"meta\"].get(\"page_number\")\n",
    "                meta_bits = []\n",
    "                if src: meta_bits.append(f\"src={src}\")\n",
    "                if page: meta_bits.append(f\"page={page}\")\n",
    "                meta_str = \" | \" + \", \".join(meta_bits) if meta_bits else \"\"\n",
    "            print(f\"  {i}. score={c['rerank_score']:.4f}  dist={c['distance']:.4f}{meta_str}\")\n",
    "    return top\n",
    "\n",
    "def retrieve_with_transform_and_rerank(query: str, collection, embedder,\n",
    "                                       k_per_query: int = 8, k_final: int = 4, verbose: bool = True):\n",
    "    \"\"\"End-to-end retrieval pipeline.\"\"\"\n",
    "    pool = retrieve_pool_with_multiquery(query, collection, embedder,\n",
    "                                         k_per_query=k_per_query, verbose=verbose)\n",
    "    topk = rerank_candidates(query, pool, top_k=k_final, verbose=verbose)\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542115a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# RESEARCH-BACKED RAG EVALUATION \n",
    "# Citations: Lewis et al. (2020), Karpukhin et al. (2020)\n",
    "# ===========================\n",
    "\n",
    "def calculate_semantic_precision_recall(query, retrieved_chunks, ground_truth_answer):\n",
    "    \"\"\"\n",
    "    Research-backed semantic precision/recall for RAG evaluation.\n",
    "    Based on: Lewis et al. (2020) - RAG paper, Karpukhin et al. (2020) - DPR paper\n",
    "    \"\"\"\n",
    "    if not retrieved_chunks or not ground_truth_answer:\n",
    "        return {'precision': 0, 'recall': 0, 'f1_score': 0}\n",
    "    \n",
    "    # Use your existing embedder\n",
    "    global embedder\n",
    "    \n",
    "    # 1. Encode all content\n",
    "    query_emb = embedder.encode(query)\n",
    "    truth_emb = embedder.encode(ground_truth_answer)\n",
    "    retrieved_embs = embedder.encode(retrieved_chunks)\n",
    "    \n",
    "    # 2. Calculate cosine similarities\n",
    "    from sentence_transformers import util\n",
    "    query_similarities = util.cos_sim(query_emb, retrieved_embs)[0]\n",
    "    truth_similarities = util.cos_sim(truth_emb, retrieved_embs)[0]\n",
    "    \n",
    "    # 3. Research standard: relevance threshold 0.6\n",
    "    relevance_threshold = 0.6\n",
    "    \n",
    "    # 4. Precision: How many retrieved chunks are relevant to query?\n",
    "    relevant_chunks = (query_similarities > relevance_threshold).sum().item()\n",
    "    precision = relevant_chunks / len(retrieved_chunks)\n",
    "    \n",
    "    # 5. Recall: Best chunk's similarity to ground truth\n",
    "    recall = truth_similarities.max().item()\n",
    "    \n",
    "    # 6. F1 Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall, \n",
    "        'f1_score': f1,\n",
    "        'avg_similarity': query_similarities.mean().item()\n",
    "    }\n",
    "\n",
    "def calculate_retrieval_metrics(questions, answers, top_k):\n",
    "    \"\"\"Calculate proper retrieval metrics for multiple questions\"\"\"\n",
    "    \n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for i, (question, answer) in enumerate(zip(questions, answers)):\n",
    "        if i >= len(answers):\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            # Get retrieval results using your existing function\n",
    "            retrieved_chunks = retrieve_relevant_chunks(question, top_k)\n",
    "            \n",
    "            # Calculate semantic metrics\n",
    "            metrics = calculate_semantic_precision_recall(question, retrieved_chunks, answer)\n",
    "            \n",
    "            precision_scores.append(metrics['precision'])\n",
    "            recall_scores.append(metrics['recall'])\n",
    "            f1_scores.append(metrics['f1_score'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {i}: {e}\")\n",
    "            precision_scores.append(0)\n",
    "            recall_scores.append(0)\n",
    "            f1_scores.append(0)\n",
    "    \n",
    "    return {\n",
    "        'precision': np.mean(precision_scores),\n",
    "        'recall': np.mean(recall_scores),\n",
    "        'f1_score': np.mean(f1_scores),\n",
    "        'precision_std': np.std(precision_scores),\n",
    "        'recall_std': np.std(recall_scores),\n",
    "        'f1_std': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "def clean_csv_data(csv_path):\n",
    "    \"\"\"Cleans the dataset for processing.\"\"\"\n",
    "    qa_df = pd.read_csv(csv_path)\n",
    "    qa_df = qa_df.dropna(subset=['Question', 'Human_Answer'])\n",
    "\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    for i, row in qa_df.iterrows():\n",
    "        q = str(row['Question']).strip()\n",
    "        a = str(row['Human_Answer']).strip()\n",
    "        \n",
    "        if q and a and q != 'nan' and a != 'nan' and len(q) > 5:\n",
    "            questions.append(q)\n",
    "            answers.append(a)\n",
    "    \n",
    "    print(f\"Cleaned data: {len(questions)} valid Q&A pairs\")\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunks_to_chromadb(chunks, batch_size=1000):\n",
    "    \"\"\"Helper to add chunks in batches.\"\"\"\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        embeddings = embedder.encode(batch_texts)\n",
    "        \n",
    "        batch_ids = [f\"chunk_{j}\" for j in range(i, i+len(batch_texts))]\n",
    "        batch_metadata = [{\"source\": f\"chunk_{j}\"} for j in range(i, i+len(batch_texts))]\n",
    "        \n",
    "        collection.add(documents=batch_texts, embeddings=embeddings.tolist(), metadatas=batch_metadata, ids=batch_ids)\n",
    "        print(f\"‚úÖ Added batch {i//batch_size + 1}/{math.ceil(len(texts)/batch_size)}\")\n",
    "    \n",
    "    print(\"All chunks successfully added to ChromaDB\")\n",
    "    return collection\n",
    "\n",
    "def test_chunk_params_semantic(dataset, documents, chunk_sizes, chunk_overlaps, top_k):\n",
    "    \"\"\"Test chunking parameters using research-backed semantic evaluation\"\"\"\n",
    "    \n",
    "    questions, answers = clean_csv_data(dataset)\n",
    "    results = []\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        for chunk_overlap in chunk_overlaps:\n",
    "            print(f\"\\nüîß Testing: chunk_size={chunk_size}, overlap={chunk_overlap}\")\n",
    "            \n",
    "            if chunk_overlap >= chunk_size:  \n",
    "                print(f\"‚ùå Skipping: overlap >= chunk_size\")\n",
    "                continue\n",
    "\n",
    "            # Setup collection with new parameters\n",
    "            clear_and_create_collection()\n",
    "            chunks = split_text(documents, chunk_size=chunk_size, overlap=chunk_overlap)\n",
    "            collection = add_chunks_to_chromadb(chunks)\n",
    "            \n",
    "            # Calculate semantic metrics (use first 20 questions for speed)\n",
    "            test_questions = questions[:20]\n",
    "            test_answers = answers[:20]\n",
    "            \n",
    "            print(f\"üìä Evaluating on {len(test_questions)} questions...\")\n",
    "            metrics = calculate_retrieval_metrics(test_questions, test_answers, top_k)\n",
    "            \n",
    "            results.append({\n",
    "                'chunk_size': chunk_size,\n",
    "                'chunk_overlap': chunk_overlap,\n",
    "                'precision': metrics['precision'],\n",
    "                'recall': metrics['recall'],\n",
    "                'f1_score': metrics['f1_score'],\n",
    "                'precision_std': metrics['precision_std'],\n",
    "                'recall_std': metrics['recall_std'],\n",
    "                'f1_std': metrics['f1_std']\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úÖ Results: P={metrics['precision']:.3f}¬±{metrics['precision_std']:.3f}, R={metrics['recall']:.3f}¬±{metrics['recall_std']:.3f}, F1={metrics['f1_score']:.3f}¬±{metrics['f1_std']:.3f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def optimize_top_k_semantic(dataset, documents, best_chunk_size, best_overlap, top_k_values):\n",
    "    \"\"\"Optimize top-k using semantic evaluation\"\"\"\n",
    "    \n",
    "    questions, answers = clean_csv_data(dataset)\n",
    "    \n",
    "    # Setup collection with best parameters\n",
    "    clear_and_create_collection()\n",
    "    chunks = split_text(documents, chunk_size=best_chunk_size, overlap=best_overlap)\n",
    "    add_chunks_to_chromadb(chunks)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for top_k in top_k_values:\n",
    "        print(f\"üîç Testing top_k={top_k}\")\n",
    "        \n",
    "        # Use first 20 questions for evaluation\n",
    "        test_questions = questions[:20]\n",
    "        test_answers = answers[:20]\n",
    "        \n",
    "        metrics = calculate_retrieval_metrics(test_questions, test_answers, top_k)\n",
    "        \n",
    "        results.append({\n",
    "            'top_k': top_k, \n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1_score': metrics['f1_score']\n",
    "        })\n",
    "        \n",
    "        print(f\"   F1: {metrics['f1_score']:.3f}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    best_config = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "\n",
    "    return results_df, best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3988e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_baseline_answer(query, model_name):\n",
    "    \"\"\"Generates an answer using only parametric knowledge.\"\"\"\n",
    "    response = ollama_client.generate(\n",
    "        model=model_name,\n",
    "        prompt=f\"\"\"You are a statistics expert. Answer the question step-by-step: \n",
    "        Question: {query}\"\"\",\n",
    "\n",
    "        stream=False,\n",
    "        options={\n",
    "        'num_predict': 1000,    # Longer responses\n",
    "        'temperature': 0.2,    # More consistent \n",
    "        'top_p': 0.9  \n",
    "    }\n",
    "    )\n",
    "    return response['response']\n",
    "\n",
    "def llm_rag_answer_simplified(query, model_name, top_k):\n",
    "    \"\"\"Generates answer using RAG context.\"\"\"\n",
    "    \n",
    "    # Get relevant chunks using simple retrieval\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, top_k)\n",
    "    \n",
    "    # Minimal context approach\n",
    "    context = relevant_chunks[:300]  # Limit context length\n",
    "    \n",
    "    # Simple, direct prompt (no complex instructions)\n",
    "    prompt = f\"\"\"You are a statistics expert. Answer the question step-by-step: \n",
    "    Statistical question: {query}\n",
    "\n",
    "Reference material (use only if directly relevant): \n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama_client.generate(\n",
    "        model=model_name,\n",
    "        prompt=prompt,\n",
    "        stream=False,\n",
    "        options={\n",
    "            'num_predict': 1000,\n",
    "            'temperature': 0.2,\n",
    "            'top_p': 0.9\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Main Dataset\n",
    "df2 = pd.read_csv(dataset_main)\n",
    "\n",
    "# Initialize Results Dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Question Topic' : df2[\"Question Type\"],\n",
    "    'Question': df2[\"Question\"],\n",
    "    'Original_Answer': df2[\"Human_Answer\"],\n",
    "\n",
    "    'Model1_Baseline': None,\n",
    "    'Model2_Baseline': None,\n",
    "    'Model3_Baseline': None,\n",
    "\n",
    "    'Model1_RAG': None,\n",
    "    'Model2_RAG': None,\n",
    "    'Model3_RAG': None,\n",
    "\n",
    "})\n",
    "\n",
    "# Models\n",
    "model1 = \"llama3.2:latest\"\n",
    "model2 = \"deepseek-r1:8b\"\n",
    "model3 = \"qwen2-math:7b\"\n",
    "\n",
    "# Use optimized top_k from previous steps (hardcoded here if optimization skipped)\n",
    "optimal_top_k = 3 # based on your results\n",
    "\n",
    "# Save parameters for reference\n",
    "import json\n",
    "optimization_params = {\n",
    "    'top_k': optimal_top_k\n",
    "}\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "print(f\"üöÄ Starting Main Experiment Loop...\")\n",
    "print(f\"   Models: {model1}, {model2}, {model3}\")\n",
    "\n",
    "for i in range(start_index, len(df)):\n",
    "    Question = df.at[i, 'Question']\n",
    "    Original_Answer = df.at[i, 'Original_Answer']\n",
    "\n",
    "    print(f\"Question {i+1}/{len(df)}:\")\n",
    "\n",
    "    # Generate responses using optimized parameters\n",
    "    llm_baseline_model1 = llm_baseline_answer(Question, model1)\n",
    "    llm_rag_model1 = llm_rag_answer_simplified(Question, model1, optimal_top_k)  \n",
    "\n",
    "    llm_baseline_model2 = llm_baseline_answer(Question, model2)\n",
    "    llm_rag_model2 = llm_rag_answer_simplified(Question, model2, optimal_top_k)  \n",
    "    \n",
    "    llm_baseline_model3 = llm_baseline_answer(Question, model3)\n",
    "    llm_rag_model3 = llm_rag_answer_simplified(Question, model3, optimal_top_k)  \n",
    "\n",
    "    # Store results\n",
    "    df.at[i, 'Question'] = Question\n",
    "    df.at[i, 'Original_Answer'] = Original_Answer\n",
    "\n",
    "    df.at[i, 'Model1_Baseline'] = llm_baseline_model1\n",
    "    df.at[i, 'Model2_Baseline'] = llm_baseline_model2\n",
    "    df.at[i, 'Model3_Baseline'] = llm_baseline_model3\n",
    "\n",
    "    df.at[i, 'Model1_RAG'] = llm_rag_model1\n",
    "    df.at[i, 'Model2_RAG'] = llm_rag_model2\n",
    "    df.at[i, 'Model3_RAG'] = llm_rag_model3\n",
    "\n",
    "    print('-' * 40)\n",
    "\n",
    "    # Save intermediate results\n",
    "    df.to_csv('results.csv', index=False)\n",
    "    \n",
    "print(\"‚úÖ Experiment completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69166ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(question, llm_answer, original_answer, judge_llm):\n",
    "    \"\"\"\n",
    "    Uses an LLM to score the answer based on Correctness, Explanation, and Understanding.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a statistics evaluator. Score the answer on the basis of its correctness, explainability and understanding. Be precise and score it between 0 and 5, also allowing for half-point increments (e.g., 2.5, 3.5).\n",
    "\n",
    "The rubrics is provided below, be very precise about your evaluations.\n",
    "RUBRIC:\n",
    "1. **CORRECTNESS**:\n",
    "    5: Correct answer AND correct method\n",
    "    4: Correct method, minor computational error\n",
    "    3: Partially correct approach\n",
    "    2: Wrong approach but shows some knowledge\n",
    "    1: Completely wrong or no attempt\n",
    "\n",
    " 2. **EXPLANATION**:\n",
    "    5: Clear, complete, step-by-step explanation\n",
    "    4: Generally clear with minor gaps\n",
    "    3: Adequate explanation with some confusion\n",
    "    2: Unclear but attempted explanation\n",
    "    1: No explanation or incomprehensible\n",
    "\n",
    " 3. **UNDERSTANDING**:\n",
    "    5: Shows deep understanding (discusses assumptions, limitations, context)\n",
    "    4: Shows good understanding with minor gaps\n",
    "    3: Shows basic understanding\n",
    "    2: Shows minimal understanding\n",
    "    1: No conceptual understanding evident\n",
    "\n",
    "---\n",
    "\n",
    "### QUESTION:\n",
    "{question}\n",
    "\n",
    "### ORIGINAL ANSWER:\n",
    "{original_answer}\n",
    "\n",
    "### LLM-GENERATED ANSWER:\n",
    "{llm_answer}\n",
    "\n",
    "---\n",
    "\n",
    "IMPORTANT: Reply with ONLY this JSON format, nothing else:\n",
    "{{{\"correctness\": X, \"explanation\": X, \"understanding\": X}}}\n",
    "\n",
    "JSON OUTPUT:\"\"\"\n",
    "    \n",
    "    response = ollama_client.generate(\n",
    "        model=judge_llm,\n",
    "        prompt=prompt,\n",
    "        stream=False,\n",
    "        options={\n",
    "            \"temperature\": 0.1,  # Low temperature for consistency\n",
    "            \"num_predict\": 50,   # Limit output length\n",
    "            'stop': ['\\n\\n', 'Human:']        # Stop after closing bracket\n",
    "        }\n",
    "    )\n",
    "\n",
    "    raw_text = response['response']\n",
    "    return raw_text\n",
    "\n",
    "def score_break(score_text):\n",
    "    \"\"\"Simple fix for parsing scores\"\"\"\n",
    "    if not score_text:\n",
    "        return 1, 1, 1\n",
    "    \n",
    "    # Just extract the numbers, ignore JSON formatting\n",
    "    import re\n",
    "    numbers = re.findall(r'\\d+', str(score_text))\n",
    "    \n",
    "    if len(numbers) >= 3:\n",
    "        return int(numbers[0]), int(numbers[1]), int(numbers[2])\n",
    "    else:\n",
    "        return 1, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20cb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llms = {\n",
    "    'Prometheus': 'vicgalle/prometheus-7b-v2.0:latest',\n",
    "    # 'Llama32': 'llama3.2:latest',\n",
    "    # 'Mistral': 'mistral:latest',\n",
    "    # 'Gemma': 'gemma3n:latest'\n",
    "}\n",
    "\n",
    "start_index = 0 \n",
    "\n",
    "print(\"‚öñÔ∏è Starting Evaluation Phase...\")\n",
    "\n",
    "for i in range(start_index, len(df)):\n",
    "    Question = df.at[i, 'Question']\n",
    "    Original_Answer = df.at[i, 'Original_Answer']\n",
    "    \n",
    "    print(f\"Row {i+1}/{len(df)}\")\n",
    "    \n",
    "    for judge_name, judge_llm in judge_llms.items():\n",
    "        print(f\"  Judge: {judge_name}\")\n",
    "        \n",
    "        for model in ['Model1', 'Model2', 'Model3']:\n",
    "            for condition in ['Baseline', 'RAG']:\n",
    "                response = df.at[i, f\"{model}_{condition}\"]\n",
    "                \n",
    "                try:\n",
    "                    # Add debugging\n",
    "                    # print(f\"    Evaluating {model}_{condition}...\")\n",
    "                    score = evaluate_answer(Question, Original_Answer, response, judge_llm)\n",
    "                    \n",
    "                    if not score or score.strip() == \"\":\n",
    "                        print(f\"    Empty score for {model}_{condition}, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    Correct, Explain, Understand = score_break(score)\n",
    "                    \n",
    "                    df.at[i, f\"{model}_{condition}_{judge_name}_Correctness\"] = Correct\n",
    "                    df.at[i, f\"{model}_{condition}_{judge_name}_Explanation\"] = Explain\n",
    "                    df.at[i, f\"{model}_{condition}_{judge_name}_Understanding\"] = Understand\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ERROR with {judge_name} on {model}_{condition}: {e}\")\n",
    "                    # Set default values on error\n",
    "                    df.at[i, f\"{model}_{condition}_{judge_name}_Correctness\"] = 1\n",
    "                    df.at[i, f\"{model}_{condition}_{judge_name}_Explanation\"] = 1\n",
    "                    df.at[i, f\"{model}_{condition}_{judge_name}_Understanding\"] = 1\n",
    "    \n",
    "    # Save after each row to prevent data loss\n",
    "    if (i + 1) % 5 == 0:\n",
    "        df.to_csv('results_progress.csv', index=False)\n",
    "        print(f\"  Saved progress at row {i+1}\")\n",
    "\n",
    "    df.to_csv('results.csv', index=False)\n",
    "    \n",
    "print(\"‚úÖ Evaluation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Multi-Judge Consensus ---\n",
    "\n",
    "judges = ['Prometheus', 'Llama32', 'Mistral', 'Gemma']\n",
    "\n",
    "print(\"=== EVALUATE BEST JUDGE ===\\n\")\n",
    "\n",
    "# Calculate consensus scores\n",
    "for model in ['Model1', 'Model2', 'Model3']:\n",
    "    for condition in ['Baseline', 'RAG']:\n",
    "        for score_type in ['Correctness', 'Explanation', 'Understanding']:\n",
    "            judge_cols = [f'{model}_{condition}_{judge}_{score_type}' for judge in judges]\n",
    "            existing_cols = [col for col in judge_cols if col in df.columns]\n",
    "            \n",
    "            if len(existing_cols) >= 2:\n",
    "                consensus_col = f'{model}_{condition}_Consensus_{score_type}'\n",
    "                df[consensus_col] = df[existing_cols].mean(axis=1)\n",
    "\n",
    "print(\"‚úÖ Consensus calculated\")\n",
    "\n",
    "# Calculate correlations\n",
    "judge_correlations = {}\n",
    "for judge in judges:\n",
    "    judge_scores = []\n",
    "    consensus_scores = []\n",
    "    \n",
    "    for model in ['Model1', 'Model2', 'Model3']:\n",
    "        for condition in ['Baseline', 'RAG']:\n",
    "            for score_type in ['Correctness', 'Explanation', 'Understanding']:\n",
    "                judge_col = f'{model}_{condition}_{judge}_{score_type}'\n",
    "                consensus_col = f'{model}_{condition}_Consensus_{score_type}'\n",
    "                \n",
    "                if judge_col in df.columns and consensus_col in df.columns:\n",
    "                    j_scores = df[judge_col].dropna().tolist()\n",
    "                    c_scores = df[consensus_col].dropna().tolist()\n",
    "                    \n",
    "                    min_len = min(len(j_scores), len(c_scores))\n",
    "                    judge_scores.extend(j_scores[:min_len])\n",
    "                    consensus_scores.extend(c_scores[:min_len])\n",
    "    \n",
    "    if len(judge_scores) > 5:\n",
    "        correlation, p_value = pearsonr(judge_scores, consensus_scores)\n",
    "        judge_correlations[judge] = correlation\n",
    "        print(f\"{judge}: correlation = {correlation:.4f}\")\n",
    "\n",
    "best_judge = max(judge_correlations, key=judge_correlations.get)\n",
    "print(f\"\\nüèÜ BEST JUDGE: {best_judge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac62ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analysis & Visualization ---\n",
    "\n",
    "# Clean up column names based on selected judge\n",
    "selected_llm_judge = \"Prometheus\"\n",
    "score_col = list()\n",
    "for col in df.columns:\n",
    "    if selected_llm_judge in col:\n",
    "        score_col.append(col)\n",
    "\n",
    "QA_col = [\"Question Topic\",\"Question\",\"Original_Answer\",'Model1_Baseline','Model2_Baseline', 'Model3_Baseline', 'Model1_RAG', 'Model2_RAG', 'Model3_RAG']\n",
    "final_col = QA_col + score_col\n",
    "\n",
    "df1 = df[final_col]\n",
    "column_names = df1.columns\n",
    "new_column_names = [col.replace(f\"_{selected_llm_judge}\", '') for col in column_names]\n",
    "df1.columns = new_column_names\n",
    "\n",
    "# Calculate Weighted Totals\n",
    "df1[\"Model1_Total_Baseline\"] = (0.5 * df1[\"Model1_Baseline_Correctness\"]) + (0.3 * df1[\"Model1_Baseline_Explanation\"]) + (0.2 * df1[\"Model1_Baseline_Understanding\"])\n",
    "df1[\"Model2_Total_Baseline\"] = (0.5 * df1[\"Model2_Baseline_Correctness\"]) + (0.3 * df1[\"Model2_Baseline_Explanation\"]) + (0.2 * df1[\"Model2_Baseline_Understanding\"])\n",
    "df1[\"Model3_Total_Baseline\"] = (0.5 * df1[\"Model3_Baseline_Correctness\"]) + (0.3 * df1[\"Model3_Baseline_Explanation\"]) + (0.2 * df1[\"Model3_Baseline_Understanding\"])\n",
    "\n",
    "df1[\"Model1_Total_RAG\"] = (0.5 * df1[\"Model1_RAG_Correctness\"]) + (0.3 * df1[\"Model1_RAG_Explanation\"]) + (0.2 * df1[\"Model1_RAG_Understanding\"])\n",
    "df1[\"Model2_Total_RAG\"] = (0.5 * df1[\"Model2_RAG_Correctness\"]) + (0.3 * df1[\"Model2_RAG_Explanation\"]) + (0.2 * df1[\"Model2_RAG_Understanding\"])\n",
    "df1[\"Model3_Total_RAG\"] = (0.5 * df1[\"Model3_RAG_Correctness\"]) + (0.3 * df1[\"Model3_RAG_Explanation\"]) + (0.2 * df1[\"Model3_RAG_Understanding\"])\n",
    "\n",
    "# Plotting\n",
    "models = ['Model1', 'Model2', 'Model3']\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "baseline = [df1[f'{m}_Total_Baseline'].mean() for m in models]\n",
    "rag = [df1[f'{m}_Total_RAG'].mean() for m in models]\n",
    "\n",
    "x = range(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar([i-width/2 for i in x], baseline, width, label='Baseline', color='blue', alpha=0.7)\n",
    "ax.bar([i+width/2 for i in x], rag, width, label='RAG', color='red', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Total Score')\n",
    "ax.set_title('Overall Performance: RAG vs Baseline')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Component-Level Analysis ---\n",
    "\n",
    "components = ['Correctness', 'Explanation', 'Understanding']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "width = 0.35\n",
    "\n",
    "for i, comp in enumerate(components):\n",
    "    baseline = [df1[f'{m}_Baseline_{comp}'].mean() for m in models]\n",
    "    rag = [df1[f'{m}_RAG_{comp}'].mean() for m in models]\n",
    "    \n",
    "    x = range(len(models))\n",
    "    axes[i].bar([j-width/2 for j in x], baseline, width, \n",
    "               label='Baseline', color='blue', alpha=0.7)\n",
    "    axes[i].bar([j+width/2 for j in x], rag, width, \n",
    "               label='RAG', color='red', alpha=0.7)\n",
    "    \n",
    "    axes[i].set_title(comp, fontweight='bold')\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(models)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    if i == 0:\n",
    "        axes[i].legend()\n",
    "\n",
    "plt.suptitle('Performance Breakdown by Evaluation Dimensions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Performance by Statistical Domain ---\n",
    "\n",
    "question_types = ['Probability', 'Hypothesis Testing', 'Regression and Correlation']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, qtype in enumerate(question_types):\n",
    "    subset = df1[df1['Question Topic'] == qtype]\n",
    "    baseline = [subset[f'{m}_Total_Baseline'].mean() for m in models]\n",
    "    rag = [subset[f'{m}_Total_RAG'].mean() for m in models]\n",
    "    \n",
    "    x = range(len(models))\n",
    "    axes[i].bar([j-width/2 for j in x], baseline, width, \n",
    "               label='Baseline', color='blue', alpha=0.7)\n",
    "    axes[i].bar([j+width/2 for j in x], rag, width, \n",
    "               label='RAG', color='red', alpha=0.7)\n",
    "    \n",
    "    axes[i].set_title(qtype, fontsize=11, fontweight='bold')\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(models, fontsize=9)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    if i == 0:\n",
    "        axes[i].legend()\n",
    "\n",
    "plt.suptitle('Performance by Statistical Domain', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62597a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary Statistics ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\nü§ñ {model}:\")\n",
    "    \n",
    "    baseline_mean = df1[f'{model}_Total_Baseline'].mean()\n",
    "    rag_mean = df1[f'{model}_Total_RAG'].mean()\n",
    "    diff = rag_mean - baseline_mean\n",
    "    pct_change = (diff / baseline_mean) * 100\n",
    "    \n",
    "    print(f\"   Baseline:  {baseline_mean:.3f}\")\n",
    "    print(f\"   RAG:       {rag_mean:.3f}\")\n",
    "    print(f\"   Change:    {diff:+.3f} ({pct_change:+.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
